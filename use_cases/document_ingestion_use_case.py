
"""

DocumentIngestionUseCase - Dok√ºman y√ºkleme i≈ülem hattƒ± (Business Logic)

PDF, DOCX, TXT, XLSX, PPTX desteƒüi

"""

import logging

from datetime import datetime

from pathlib import Path

from typing import List
 
from app_refactored.core.interfaces import (

    IEmbeddingService,

    IDocumentRepository

)

from app_refactored.core.entities import (

    DocumentChunk,

    DocumentIngestionResult

)

from .document_preprocessing import DocumentPreprocessor
from .intelligent_chunking import IntelligentChunker
from .html_structured_converter import HTMLStructuredConverter
 
logger = logging.getLogger(__name__)
 
 
class DocumentIngestionUseCase:

    """Dok√ºman y√ºkleme use case - Dependency Injection ile"""
 
    def __init__(

        self,

        embedding_service: IEmbeddingService,

        document_repository: IDocumentRepository,

        chunk_size: int = 1000,

        chunk_overlap: int = 200

    ):

        """

        Initialize document ingestion use case

        Args:

            embedding_service: IEmbeddingService implementation

            document_repository: IDocumentRepository implementation

            chunk_size: Chunk boyutu (karakterler)

            chunk_overlap: Chunk'lar arasƒ± overlap

        """

        self.embedding_service = embedding_service

        self.document_repository = document_repository

        self.chunk_size = chunk_size

        self.chunk_overlap = chunk_overlap
 
    def _extract_text(self, file_path: str) -> str:

        """Dosya t√ºr√ºne g√∂re metni √ßƒ±kart"""

        path = Path(file_path)

        file_type = path.suffix.lower()
 
        try:

            if file_type == '.pdf':

                return self._extract_pdf(file_path)

            elif file_type == '.docx':

                return self._extract_docx(file_path)

            elif file_type == '.txt':

                return self._extract_txt(file_path)

            elif file_type in ('.html', '.html'):
                return self._extract_html(file_path)

            elif file_type == '.xlsx':

                return self._extract_xlsx(file_path)

            elif file_type == '.pptx':

                return self._extract_pptx(file_path)

            else:

                raise ValueError(f"Unsupported file type: {file_type}")

        except Exception as e:

            logger.error(f"‚ùå Text extraction failed: {str(e)}")

            raise
 
    def _extract_pdf(self, file_path: str) -> str:

        """PDF'den metin √ßƒ±kart - pdfplumber (tablo destekli) + PyMuPDF fallback"""

        try:

            import pdfplumber

            pages_text = []

            with pdfplumber.open(file_path) as pdf:

                for page_num, page in enumerate(pdf.pages):

                    page_parts = []

                    page_parts.append(f"## Sayfa {page_num + 1}")

                    # Tablolarƒ± √ßƒ±kar

                    tables = page.extract_tables()

                    if tables:

                        for table in tables:

                            for row in table:

                                cells = [str(c).strip() if c else "" for c in row]

                                non_empty = [c for c in cells if c]
                                #Key:Value √ßiftlerini tespit et
                                i = 0
                                while i < len(non_empty):
                                    cell = non_empty[i]
                                    if cell.endswith(':') and i + 1 < len(non_empty):
                                        page_parts.append(f"{cell} {non_empty[i+1]}")
                                        i += 2
                                    elif ':' not in cell and i == 0 and len(non_empty) == 2:
                                        page_parts.append(f"{non_empty[0]}: {non_empty[1]}")
                                        i = len(non_empty)
                                    else:
                                        page_parts.append(cell)
                                        i += 1

                                
                    else:

                        text = page.extract_text()

                        if text:

                            page_parts.append(text.strip())

                    if len(page_parts) > 1:

                        pages_text.append("\n".join(page_parts))

            full_text = "\n\n".join(pages_text)

            if len(full_text.strip()) > 100:

                logger.info(f"‚úÖ PDF pdfplumber extraction: {len(full_text)} chars, {len(pages_text)} pages")

                return full_text

            # pdfplumber yetersizse PyMuPDF fallback

            logger.warning(f"‚ö†Ô∏è pdfplumber yetersiz ({len(full_text)} chars), PyMuPDF deneniyor...")

            import fitz

            doc = fitz.open(file_path)

            pages_text = []

            for page_num, page in enumerate(doc):

                text = page.get_text("text")

                if text and text.strip():

                    pages_text.append(f"## Sayfa {page_num + 1}\n\n{text.strip()}")

            doc.close()

            full_text = "\n\n".join(pages_text)

            logger.info(f"‚úÖ PDF PyMuPDF fallback: {len(full_text)} chars")

            return full_text

        except Exception as e:

            logger.error(f"‚ùå PDF extraction failed: {str(e)}")

            raise
 
    def _extract_docx(self, file_path: str) -> str:

        """DOCX'den metin √ßƒ±kart"""

        try:

            from docx import Document

            doc = Document(file_path)

            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])

            logger.info(f"‚úÖ DOCX extraction: {len(text)} chars")

            return text

        except Exception as e:

            logger.error(f"‚ùå DOCX extraction failed: {str(e)}")

            raise
 
    def _extract_txt(self, file_path: str) -> str:

        """TXT'den metin √ßƒ±kart"""

        try:

            with open(file_path, 'r', encoding='utf-8') as file:

                text = file.read()

            logger.info(f"‚úÖ TXT extraction: {len(text)} chars")

            return text

        except Exception as e:

            logger.error(f"‚ùå TXT extraction failed: {str(e)}")

            raise
 
    def _extract_xlsx(self, file_path: str) -> str:

        """XLSX'den metin √ßƒ±kart"""

        try:

            import openpyxl

            wb = openpyxl.load_workbook(file_path)

            text = ""

            for sheet in wb.sheetnames:

                ws = wb[sheet]

                for row in ws.iter_rows(values_only=True):

                    text += " ".join(str(cell) if cell else "" for cell in row) + "\n"

            logger.info(f"‚úÖ XLSX extraction: {len(text)} chars")

            return text

        except Exception as e:

            logger.error(f"‚ùå XLSX extraction failed: {str(e)}")

            raise
 
    def _extract_pptx(self, file_path: str) -> str:

        """PPTX'den metin √ßƒ±kart"""

        try:

            from pptx import Presentation

            prs = Presentation(file_path)

            text = ""

            for slide in prs.slides:

                for shape in slide.shapes:

                    if hasattr(shape, "text"):

                        text += shape.text + "\n"

            logger.info(f"‚úÖ PPTX extraction: {len(text)} chars")

            return text

        except Exception as e:

            logger.error(f"‚ùå PPTX extraction failed: {str(e)}")

            raise

    def _extract_html(self, file_path: str) -> str:

        """HTML'den metin √ßƒ±kart - HTMLStructuredConverter ile yapƒ±sal parse"""

        try:

            html_content = None

            for enc in ('utf-8', 'windows-1254', 'latin-1', 'iso-8859-9'):

                try:

                    with open(file_path, 'r', encoding=enc) as file:

                        html_content = file.read()

                    logger.info(f"üìÑ HTML decoded with encoding: {enc}")

                    break

                except UnicodeDecodeError:

                    continue

            if html_content is None:

                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:

                    html_content = file.read()

            converter = HTMLStructuredConverter()

            result = converter.convert(html_content)

            logger.info(f"‚úÖ HTML extraction: {len(result)} chars")

            return result

        except Exception as e:

            logger.error(f"‚ùå HTML extraction failed: {str(e)}")

            raise
 
    
    def _chunk_text(self, text: str) -> List[str]:

        """Metni chunk'lara b√∂l"""

        chunks = []

        start = 0

        while start < len(text):

            end = start + self.chunk_size

            chunk = text[start:end]

            chunks.append(chunk)

            start += (self.chunk_size - self.chunk_overlap)

        logger.info(f"‚úÖ Text chunked: {len(chunks)} chunks")

        return chunks
 
    async def execute(self, file_path: str, filename: str) -> DocumentIngestionResult:

        """

        Execute document ingestion pipeline

        Args:

            file_path: Dosyanƒ±n tam yolu

            filename: Orijinal dosya adƒ±

        Returns:

            DocumentIngestionResult

        """

        try:

            logger.info(f"üì• Ingesting: {filename}")

            # Step 0: Aynƒ± dosyanƒ±n eski chunk'larƒ±nƒ± temizle (re-upload desteƒüi)
            try:
                deleted_count = await self.document_repository.delete_by_filename(filename)
                if deleted_count > 0:
                    logger.info(f"üóëÔ∏è Deleted {deleted_count} old chunks for {filename}")
            except Exception as del_err:
                logger.warning(f"‚ö†Ô∏è Old chunk cleanup failed (continuing): {del_err}")
 
            # Step 1: Metni √ßƒ±kart

            logger.info(f"üìÑ Extracting text from {Path(filename).suffix}...")

            text = self._extract_text(file_path)

            original_length = len(text)

            logger.info("üßπ Preprocessing text...")
            preprocessor = DocumentPreprocessor()
            raw_type = Path(filename).suffix.lstrip('.')
            #HTML zaten extract_html'de temizlendi, d√ºz metin olarak i≈üle
            preprocess_type = "txt" if raw_type in ("html", "htm") else raw_type
            text = preprocessor.preprocess(
                text,
                file_type=preprocess_type
            )
 
            # Step 2: Chunk'la

            logger.info("‚úÇÔ∏è Intelligent chunking...")

            chunker = IntelligentChunker(chunk_size=1000, chunk_overlap=200)
            chunk_objects = chunker.chunk(text)
            chunks = [c['content'] for c in chunk_objects]
            #Kaynak prefix ekle
            doc_label = filename.rsplit(".", 1)[0].replace("-", " ").replace("_", " ").title()
            chunks = [f"[Kaynak: {doc_label}]\n\n{c}" for c in chunks]
 
            # Step 3: Embedding'ler olu≈ütur

            logger.info("üìä Generating embeddings...")

            embeddings = await self.embedding_service.embed_batch(chunks)
 
            if len(embeddings) != len(chunks):

                raise Exception(f"Embedding count mismatch: {len(embeddings)} vs {len(chunks)}")
 
            # Step 4: Veritabanƒ±na kaydet

            logger.info("üíæ Saving to database...")

            documents = [

                DocumentChunk(

                    filename=filename,

                    chunk_index=idx,

                    content=chunk,

                    embedding=embedding,

                    metadata={

                        "file_type": Path(filename).suffix,

                        "chunk_size": len(chunk),

                        "original_length": original_length,

                        "header": chunk_objects[idx].get('header'),

                        "source": filename,

                        "chunk_position": chunk_objects[idx].get('position')

                    }

                )

                for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings))

            ]
 
            saved_docs = await self.document_repository.save_batch(documents)
 
            # Token sayƒ±sƒ± (rough estimate: 1 token ‚âà 4 chars)

            total_tokens = original_length // 4
 
            logger.info(f"‚úÖ Ingestion completed: {len(saved_docs)} chunks, {total_tokens} tokens")
 
            return DocumentIngestionResult(

                status="success",

                filename=filename,

                chunks_ingested=len(saved_docs),

                total_tokens=total_tokens,

                timestamp=datetime.now()

            )
 
        except Exception as e:

            logger.error(f"‚ùå Document ingestion failed: {str(e)}")

            return DocumentIngestionResult(

                status="error",

                filename=filename,

                chunks_ingested=0,

                total_tokens=0,

                timestamp=datetime.now(),

                error=str(e)

            )
 
