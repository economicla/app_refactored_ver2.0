"""
RAGQueryUseCase - Advanced Soru Sorma Ä°ÅŸlem HattÄ±
Business Logic - Framework baÄŸÄ±msÄ±z
Advanced preprocessing, intelligent chunking, source tracking ve compliance
"""

import logging
from typing import Optional, List
from datetime import datetime
from dataclasses import dataclass

from app_refactored.core.interfaces import (
    IEmbeddingService,
    IDocumentRepository,
    ILLMService
)

from app_refactored.core.entities import (
    RAGQuery,
    RAGResponse,
    DocumentChunk
)

logger = logging.getLogger(__name__)


@dataclass
class SourceWithMetadata:
    """Kaynak bilgisi (metadata ile)"""
    filename: str
    chunk_index: int
    header: Optional[str]
    similarity_score: float
    content_preview: str
    chunk_size: int


class RAGQueryUseCase:
    """
    Advanced RAG Query Use Case
    1. Query embedding
    2. Semantic search with source tracking
    3. System prompt + Banking compliance
    4. LLM generation with temperature=0 constraints
    5. Detailed source attribution
    """
    
    # Banka Mevzuat Analisti System Prompt
    SYSTEM_PROMPT = """Kimsin: 

- Sen, Åirketlerin finansal verilerini analiz ederek ÅŸirketler hakkÄ±nda sorulan tÃ¼m finansal sorulara yanÄ±t verebilen profesyonel bir kredi destek asistanÄ±sÄ±n. Senin kullanÄ±cÄ±larÄ±n ALBARAKA bankasÄ±nÄ±n yÃ¶netim kurulunun Ã¼yeleridir.

GÃ¶revin:
-KullanÄ±cÄ±lar sana gruplar ve gruba baÄŸlÄ± firmalar hakkÄ±nda sana sorular soracaklar. AmaÃ§larÄ± firmalardan Albaraka'ya (senin de sanal Ã§alÄ±ÅŸanÄ± olduÄŸun Banka) gelen kredi taleplerini deÄŸelendirmek. Senle yaptÄ±klarÄ± yazÄ±ÅŸmalar sonucunda firmalarÄ±n kredi arttÄ±rÄ±p taleplerini karar verecekler. O yÃ¼zden senin vermiÅŸ olduÄŸun bilgiler Ã§ok kritik. Olumu olumsuz bu kararÄ± verirken en Ã¶nemli destekleyicileri sensin.

TEMEL KURALLAR:
1. SADECE verilen dokÃ¼mantasyondaki bilgiyi kullan
2. Bilgi bulunamazsa kesin olarak: "Bilgi mevcut deÄŸil" yaz
3. Asla spekÃ¼lasyon yapma veya tahmin etme
4. CevaplarÄ± kesin, net ve profesyonel yap
5. Her cevabÄ±n sonunda kaynak bilgisini ekle
6. DÃ¼nyanÄ±n en iyi kredi uzmanÄ± gibi cevaplarÄ±nÄ± Ã¼ret.
7. Soruyu Ä°ngilizce sorarsam Ä°ngilizce cevap ver. TÃ¼rkÃ§e sorarsam TÃ¼rkÃ§e cevap ver.
7. Kolay anlaÅŸÄ±lÄ±r Ã§Ä±ktÄ±lar Ã¼ret.
8. Profesyonel bir biÃ§imde yanÄ±t ver, kullandÄ±ÄŸÄ±n dil resmi bir dil olsun.

Ã‡IKTI FORMATI:
CEVAP: [DetaylÄ± ve kesin yanÄ±t]
KAYNAKLAR: 
  - [Dosya: dosya_adÄ±]
  - [BÃ¶lÃ¼m: baÅŸlÄ±k]
  - [GÃ¼ven: YÃ¼ksek/Orta/DÃ¼ÅŸÃ¼k]

UYARI: EÄŸer sorulmuÅŸ konuda dÃ¶kÃ¼man bulunamazsa, her zaman "Bilgi mevcut deÄŸil" yaz."""

    def __init__(
        self,
        embedding_service: IEmbeddingService,
        document_repository: IDocumentRepository,
        llm_service: ILLMService
    ):
        """
        Initialize Advanced RAG use case with dependencies
        
        Args:
            embedding_service: IEmbeddingService implementation
            document_repository: IDocumentRepository implementation
            llm_service: ILLMService implementation
        """
        self.embedding_service = embedding_service
        self.document_repository = document_repository
        self.llm_service = llm_service

    async def execute(self, query: RAGQuery) -> RAGResponse:
        """
        Execute advanced RAG query pipeline with source tracking
        
        Args:
            query: RAGQuery nesnesi (sorgu, top_k, temperature, user_id)
        
        Returns:
            RAGResponse: DetaylÄ± cevap + kaynak atÄ±flamasÄ±
        """
        
        try:
            logger.info(f"ğŸ” Processing query: {query.query[:50]}... [User: {query.user_id}]")

            # Step 1: Sorguyu embedding'e Ã§evir
            logger.info("ğŸ“Š Embedding query...")
            query_embedding = await self.embedding_service.embed_text(query.query)
            
            if not query_embedding:
                raise Exception("Embedding oluÅŸturulamadÄ±")

            # Step 2: Benzer dokÃ¼mantasyonu ara (semantic search)
            logger.info(f"ğŸ” Searching similar documents (top_k={query.top_k})...")
            search_result = await self.document_repository.search_similar(
                embedding=query_embedding,
                top_k=query.top_k,
                threshold=0.0
            )
            
            # SonuÃ§ yoksa
            if not search_result.documents:
                logger.warning("âš ï¸ No similar documents found")
                return RAGResponse(
                    question=query.query,
                    answer="Sorgunuzla ilgili dÃ¶kÃ¼man bulunamadÄ±.",
                    sources=[],
                    model=await self.llm_service.get_model_name(),
                    timestamp=datetime.utcnow(),
                    user_id=query.user_id
                )

            # Step 3: Kontekst oluÅŸtur + Kaynak izle (source tracking)
            logger.info(f"ğŸ“ Building context from {len(search_result.documents)} chunks...")
            context_parts = []
            sources_with_metadata: List[SourceWithMetadata] = []
            
            for idx, doc in enumerate(search_result.documents):
                # Metadata'dan baÅŸlÄ±k bilgisini al
                header = None
                if hasattr(doc, 'metadata') and doc.metadata:
                    header = doc.metadata.get('header')
                
                # Context'e ekle
                header_text = f" [{header}]" if header else ""
                context_parts.append(
                    f"[Kaynak {idx+1}: {doc.filename}{header_text}]\n{doc.content}"
                )
                
                # DetaylÄ± kaynak bilgisi
                similarity_score = getattr(doc, 'similarity_score', 0)
                content_preview = doc.content[:150] + "..." if len(doc.content) > 150 else doc.content
                
                sources_with_metadata.append(
                    SourceWithMetadata(
                        filename=doc.filename,
                        chunk_index=doc.chunk_index,
                        header=header,
                        similarity_score=similarity_score,
                        content_preview=content_preview,
                        chunk_size=len(doc.content)
                    )
                )
            
            context = "\n\n---\n\n".join(context_parts)

            # Step 4: Prompt oluÅŸtur (System prompt + Constraints)
            logger.info("ğŸ“ Building prompt with banking compliance constraints...")
            prompt = f"""KONTEXT (YalnÄ±zca aÅŸaÄŸÄ±daki bilgiyi kullan):
{context}

SORU: {query.query}

YANIT (kesin, kaynaklÄ± ve profesyonel):"""

            # Step 5: LLM'den yanÄ±t al (Temperature=0 - kesin yanÄ±tlar)
            logger.info("ğŸ¤– Generating response from LLM (temperature=0)...")
            answer = await self.llm_service.generate_response(
                prompt=prompt,
                system_prompt=self.SYSTEM_PROMPT,
                temperature=0,  # KESIN YANITLAR (query.temperature'den baÄŸÄ±msÄ±z)
                max_tokens=2000
            )

            logger.info(f"âœ… Advanced RAG query successful [User: {query.user_id}]")

            # YanÄ±t dÃ¶ndÃ¼r (source tracking ile)
            return RAGResponse(
                question=query.query,
                answer=answer.strip(),
                sources=sources_with_metadata,
                model=await self.llm_service.get_model_name(),
                timestamp=datetime.utcnow(),
                user_id=query.user_id
            )

        except Exception as e:
            logger.error(f"âŒ Advanced RAG query failed: {str(e)}")
            raise

    async def stream_query(self, query: RAGQuery):
        """
        Execute advanced RAG query with streaming response + source tracking
        Real-time cevap almak iÃ§in
        
        Yields:
            str: AkÄ±ÅŸ halinde cevap parÃ§alarÄ±
        """
        
        try:
            logger.info(f"ğŸ” Processing stream query: {query.query[:50]}... [User: {query.user_id}]")

            # Step 1: Sorguyu embedding'e Ã§evir
            query_embedding = await self.embedding_service.embed_text(query.query)
            
            if not query_embedding:
                raise Exception("Embedding oluÅŸturulamadÄ±")

            # Step 2: Benzer dokÃ¼mantasyonu ara
            logger.info(f"ğŸ” Searching similar documents (top_k={query.top_k})...")
            search_result = await self.document_repository.search_similar(
                embedding=query_embedding,
                top_k=query.top_k,
                threshold=0.0
            )
            
            if not search_result.documents:
                logger.warning("âš ï¸ No similar documents found for stream")
                yield "Sorgunuzla ilgili dÃ¶kÃ¼man bulunamadÄ±."
                return

            # Step 3: Kontekst oluÅŸtur + Kaynak izle
            logger.info(f"ğŸ“ Building context from {len(search_result.documents)} chunks...")
            context_parts = []
            sources_with_metadata: List[SourceWithMetadata] = []
            
            for idx, doc in enumerate(search_result.documents):
                # Metadata'dan baÅŸlÄ±k bilgisini al
                header = None
                if hasattr(doc, 'metadata') and doc.metadata:
                    header = doc.metadata.get('header')
                
                # Context'e ekle
                header_text = f" [{header}]" if header else ""
                context_parts.append(
                    f"[Kaynak {idx+1}: {doc.filename}{header_text}]\n{doc.content}"
                )
                
                # DetaylÄ± kaynak bilgisi
                similarity_score = getattr(doc, 'similarity_score', 0)
                content_preview = doc.content[:150] + "..." if len(doc.content) > 150 else doc.content
                
                sources_with_metadata.append(
                    SourceWithMetadata(
                        filename=doc.filename,
                        chunk_index=doc.chunk_index,
                        header=header,
                        similarity_score=similarity_score,
                        content_preview=content_preview,
                        chunk_size=len(doc.content)
                    )
                )
            
            context = "\n\n---\n\n".join(context_parts)

            # Step 4: Prompt oluÅŸtur
            prompt = f"""KONTEXT (YalnÄ±zca aÅŸaÄŸÄ±daki bilgiyi kullan):
{context}

SORU: {query.query}

YANIT (kesin, kaynaklÄ± ve profesyonel):"""

            # Step 5: LLM'den stream yanÄ±t al (Temperature=0)
            logger.info("ğŸ¤– Generating stream response from LLM (temperature=0)...")
            
            # Kaynak bilgisini ilk olarak gÃ¶nder
            yield f"ğŸ“š KAYNAKLAR ({len(sources_with_metadata)} chunk):\n"
            for source in sources_with_metadata:
                header_info = f" - {source.header}" if source.header else ""
                yield f"  â€¢ {source.filename}{header_info} (Similarity: {source.similarity_score:.2f})\n"
            yield "\n" + "="*70 + "\n\n"
            
            # Stream cevaplarÄ± gÃ¶nder
            async for chunk in self.llm_service.stream_response(
                prompt=prompt,
                system_prompt=self.SYSTEM_PROMPT,
                temperature=0,  # KESIN YANITLAR
                max_tokens=2000
            ):
                yield chunk

            logger.info(f"âœ… Advanced stream RAG query successful [User: {query.user_id}]")

        except Exception as e:
            logger.error(f"âŒ Stream RAG query failed: {str(e)}")
            raise
