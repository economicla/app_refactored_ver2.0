"""
RAGQueryUseCase - Advanced Soru Sorma ƒ∞≈ülem Hattƒ±
Business Logic - Framework baƒüƒ±msƒ±z
Advanced preprocessing, intelligent chunking, source tracking ve compliance
"""

import logging
from typing import Optional, List
from datetime import datetime
from dataclasses import dataclass

from app_refactored.core.interfaces import (
    IEmbeddingService,
    IDocumentRepository,
    ILLMService
)

from app_refactored.core.entities import (
    RAGQuery,
    RAGResponse,
    DocumentChunk
)

logger = logging.getLogger(__name__)


@dataclass
class SourceWithMetadata:
    """Kaynak bilgisi (metadata ile)"""
    filename: str
    chunk_index: int
    header: Optional[str]
    similarity_score: float
    content_preview: str
    chunk_size: int


class RAGQueryUseCase:
    """
    Advanced RAG Query Use Case
    1. Query embedding
    2. Semantic search with source tracking
    3. System prompt + Banking compliance
    4. LLM generation with temperature=0 constraints
    5. Detailed source attribution
    """
    
    # Banka Mevzuat Analisti System Prompt
    SYSTEM_PROMPT = """Kimsin: 

- Sen, ≈ûirketlerin finansal verilerini analiz ederek ≈üirketler hakkƒ±nda sorulan t√ºm finansal sorulara yanƒ±t verebilen profesyonel bir kredi destek asistanƒ±sƒ±n. Senin kullanƒ±cƒ±larƒ±n ALBARAKA bankasƒ±nƒ±n y√∂netim kurulunun √ºyeleridir.

G√∂revin:
-Kullanƒ±cƒ±lar sana gruplar ve gruba baƒülƒ± firmalar hakkƒ±nda sana sorular soracaklar. Ama√ßlarƒ± firmalardan Albaraka'ya (senin de sanal √ßalƒ±≈üanƒ± olduƒüun Banka) gelen kredi taleplerini deƒüelendirmek. Senle yaptƒ±klarƒ± yazƒ±≈ümalar sonucunda firmalarƒ±n kredi arttƒ±rƒ±p taleplerini karar verecekler. O y√ºzden senin vermi≈ü olduƒüun bilgiler √ßok kritik. Olumu olumsuz bu kararƒ± verirken en √∂nemli destekleyicileri sensin.

VERƒ∞ FORMATI:
Kontekstteki veriler "kalem: d√∂nem1 d√∂neminde deƒüer1, d√∂nem2 d√∂neminde deƒüer2" ≈üeklinde d√ºzenlenmi≈ütir.
√ñrneƒüin: "AKTƒ∞F TOPLAMI: 2023/12 d√∂neminde 1.246.915.353, 2024/12 d√∂neminde 1.137.605.159"
Bu formatta d√∂nem ve deƒüer bilgilerini dikkatle oku ve soruyu cevapla.

TEMEL KURALLAR:
1. SADECE verilen kontekstteki bilgiyi kullan
2. Kontekstte sayƒ±sal veriler, d√∂nem bilgileri veya ilgili kalemler VARSA kesinlikle cevapla - "Bilgi mevcut deƒüil" YAZMA
3. Asla spek√ºlasyon yapma veya tahmin etme
4. Cevaplarƒ± kesin, net ve profesyonel yap
5. Her cevabƒ±n sonunda kaynak bilgisini ekle
6. D√ºnyanƒ±n en iyi kredi uzmanƒ± gibi cevaplarƒ±nƒ± √ºret
7. Soruyu ƒ∞ngilizce sorarsam ƒ∞ngilizce cevap ver. T√ºrk√ße sorarsam T√ºrk√ße cevap ver
8. Kolay anla≈üƒ±lƒ±r √ßƒ±ktƒ±lar √ºret
9. Profesyonel bir bi√ßimde yanƒ±t ver, kullandƒ±ƒüƒ±n dil resmi bir dil olsun
10. D√∂nemler arasƒ± kar≈üƒ±la≈ütƒ±rma istendiƒüinde, deƒüerleri tablo veya liste halinde sun

√áIKTI FORMATI:
CEVAP: [Detaylƒ± ve kesin yanƒ±t]
KAYNAKLAR: 
  - [Dosya: dosya_adƒ±]
  - [B√∂l√ºm: ba≈ülƒ±k]
  - [G√ºven: Y√ºksek/Orta/D√º≈ü√ºk]

UYARI: SADECE kontekstte soruyla hi√ß ilgili veri bulunmadƒ±ƒüƒ±nda "Bilgi mevcut deƒüil" yaz. Eƒüer kontekstte herhangi bir sayƒ±sal veri veya d√∂nem bilgisi varsa, onu kullanarak mutlaka cevap ver."""

    def __init__(
        self,
        embedding_service: IEmbeddingService,
        document_repository: IDocumentRepository,
        llm_service: ILLMService
    ):
        """
        Initialize Advanced RAG use case with dependencies
        
        Args:
            embedding_service: IEmbeddingService implementation
            document_repository: IDocumentRepository implementation
            llm_service: ILLMService implementation
        """
        self.embedding_service = embedding_service
        self.document_repository = document_repository
        self.llm_service = llm_service

    async def execute(self, query: RAGQuery) -> RAGResponse:
        """
        Execute advanced RAG query pipeline with source tracking
        
        Args:
            query: RAGQuery nesnesi (sorgu, top_k, temperature, user_id)
        
        Returns:
            RAGResponse: Detaylƒ± cevap + kaynak atƒ±flamasƒ±
        """
        
        try:
            logger.info(f"üîç Processing query: {query.query[:50]}... [User: {query.user_id}]")

            # Step 1: Sorguyu embedding'e √ßevir
            logger.info("üìä Embedding query...")
            query_embedding = await self.embedding_service.embed_text(query.query)
            
            if not query_embedding:
                raise Exception("Embedding olu≈üturulamadƒ±")

            # Step 2: Benzer dok√ºmantasyonu ara (semantic search)
            logger.info(f"üîé Searching similar documents (top_k={query.top_k})...")
            search_result = await self.document_repository.search_similar(
                embedding=query_embedding,
                top_k=query.top_k,
                threshold=0.0
            )
            
            # Sonu√ß yoksa
            if not search_result.documents:
                logger.warning("‚ö†Ô∏è No similar documents found")
                return RAGResponse(
                    question=query.query,
                    answer="Sorgunuzla ilgili d√∂k√ºman bulunamadƒ±.",
                    sources=[],
                    model=await self.llm_service.get_model_name(),
                    timestamp=datetime.utcnow(),
                    user_id=query.user_id
                )

            # Step 3: Kontekst olu≈ütur + Kaynak izle (source tracking)
            logger.info(f"üìù Building context from {len(search_result.documents)} chunks...")
            context_parts = []
            sources_with_metadata: List[SourceWithMetadata] = []
            
            for idx, doc in enumerate(search_result.documents):
                # Metadata'dan ba≈ülƒ±k bilgisini al
                header = None
                if hasattr(doc, 'metadata') and doc.metadata:
                    header = doc.metadata.get('header')
                
                # DEBUG: ƒ∞lk chunk'ƒ±n i√ßeriƒüini logla
                if idx == 0:
                    logger.info(f"üìã Top chunk [{doc.filename}] header={header} "
                                f"sim={getattr(doc, 'similarity_score', 0):.3f} "
                                f"len={len(doc.content)} "
                                f"preview={doc.content[:300]}")
                
                # Context'e ekle
                header_text = f" [{header}]" if header else ""
                context_parts.append(
                    f"[Kaynak {idx+1}: {doc.filename}{header_text}]\n{doc.content}"
                )
                
                # Detaylƒ± kaynak bilgisi
                similarity_score = getattr(doc, 'similarity_score', 0)
                content_preview = doc.content[:150] + "..." if len(doc.content) > 150 else doc.content
                
                sources_with_metadata.append(
                    SourceWithMetadata(
                        filename=doc.filename,
                        chunk_index=doc.chunk_index,
                        header=header,
                        similarity_score=similarity_score,
                        content_preview=content_preview,
                        chunk_size=len(doc.content)
                    )
                )
            
            context = "\n\n---\n\n".join(context_parts)

            # Step 4: Prompt olu≈ütur (System prompt + Constraints)
            logger.info("üìù Building prompt with banking compliance constraints...")
            prompt = f"""KONTEXT (A≈üaƒüƒ±daki finansal verileri dikkatlice oku ve soruyu cevapla):
{context}

SORU: {query.query}

YANIT (kesin, kaynaklƒ± ve profesyonel):"""

            # Step 5: LLM'den yanƒ±t al (Temperature=0 - kesin yanƒ±tlar)
            logger.info("ü§ñ Generating response from LLM (temperature=0)...")
            answer = await self.llm_service.generate_response(
                prompt=prompt,
                system_prompt=self.SYSTEM_PROMPT,
                temperature=0,  # KESIN YANITLAR (query.temperature'den baƒüƒ±msƒ±z)
                max_tokens=2000
            )

            logger.info(f"‚úÖ Advanced RAG query successful [User: {query.user_id}]")

            # Yanƒ±t d√∂nd√ºr (source tracking ile)
            return RAGResponse(
                question=query.query,
                answer=answer.strip(),
                sources=sources_with_metadata,
                model=await self.llm_service.get_model_name(),
                timestamp=datetime.utcnow(),
                user_id=query.user_id
            )

        except Exception as e:
            logger.error(f"‚ùå Advanced RAG query failed: {str(e)}")
            raise

    async def stream_query(self, query: RAGQuery):
        """
        Execute advanced RAG query with streaming response + source tracking
        Real-time cevap almak i√ßin
        
        Yields:
            str: Akƒ±≈ü halinde cevap par√ßalarƒ±
        """
        
        try:
            logger.info(f"üîç Processing stream query: {query.query[:50]}... [User: {query.user_id}]")

            # Step 1: Sorguyu embedding'e √ßevir
            query_embedding = await self.embedding_service.embed_text(query.query)
            
            if not query_embedding:
                raise Exception("Embedding olu≈üturulamadƒ±")

            # Step 2: Benzer dok√ºmantasyonu ara
            logger.info(f"üîé Searching similar documents (top_k={query.top_k})...")
            search_result = await self.document_repository.search_similar(
                embedding=query_embedding,
                top_k=query.top_k,
                threshold=0.0
            )
            
            if not search_result.documents:
                logger.warning("‚ö†Ô∏è No similar documents found for stream")
                yield "Sorgunuzla ilgili d√∂k√ºman bulunamadƒ±."
                return

            # Step 3: Kontekst olu≈ütur + Kaynak izle
            logger.info(f"üìù Building context from {len(search_result.documents)} chunks...")
            context_parts = []
            sources_with_metadata: List[SourceWithMetadata] = []
            
            for idx, doc in enumerate(search_result.documents):
                # Metadata'dan ba≈ülƒ±k bilgisini al
                header = None
                if hasattr(doc, 'metadata') and doc.metadata:
                    header = doc.metadata.get('header')
                
                # Context'e ekle
                header_text = f" [{header}]" if header else ""
                context_parts.append(
                    f"[Kaynak {idx+1}: {doc.filename}{header_text}]\n{doc.content}"
                )
                
                # Detaylƒ± kaynak bilgisi
                similarity_score = getattr(doc, 'similarity_score', 0)
                content_preview = doc.content[:150] + "..." if len(doc.content) > 150 else doc.content
                
                sources_with_metadata.append(
                    SourceWithMetadata(
                        filename=doc.filename,
                        chunk_index=doc.chunk_index,
                        header=header,
                        similarity_score=similarity_score,
                        content_preview=content_preview,
                        chunk_size=len(doc.content)
                    )
                )
            
            context = "\n\n---\n\n".join(context_parts)

            # Step 4: Prompt olu≈ütur
            prompt = f"""KONTEXT (A≈üaƒüƒ±daki finansal verileri dikkatlice oku ve soruyu cevapla):
{context}

SORU: {query.query}

YANIT (kesin, kaynaklƒ± ve profesyonel):"""

            # Step 5: LLM'den stream yanƒ±t al (Temperature=0)
            logger.info("ü§ñ Generating stream response from LLM (temperature=0)...")
            
            # Kaynak bilgisini ilk olarak g√∂nder
            yield f"üìö KAYNAKLAR ({len(sources_with_metadata)} chunk):\n"
            for source in sources_with_metadata:
                header_info = f" - {source.header}" if source.header else ""
                yield f"  ‚Ä¢ {source.filename}{header_info} (Similarity: {source.similarity_score:.2f})\n"
            yield "\n" + "="*70 + "\n\n"
            
            # Stream cevaplarƒ± g√∂nder
            async for chunk in self.llm_service.stream_response(
                prompt=prompt,
                system_prompt=self.SYSTEM_PROMPT,
                temperature=0,  # KESIN YANITLAR
                max_tokens=2000
            ):
                yield chunk

            logger.info(f"‚úÖ Advanced stream RAG query successful [User: {query.user_id}]")

        except Exception as e:
            logger.error(f"‚ùå Stream RAG query failed: {str(e)}")
            raise
